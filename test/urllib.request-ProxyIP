import json, time,random,urllib.request
from urllib.request import urlopen, Request
import pandas as pd
import numpy as np


url = 'http://study.163.com/p/search/studycourse.json'


#
# print(data)

headers = {
    'Content-Type': 'application/json',
    'edu-script-token': 'd5cba8e3d4b44d01ba70d89cdbbebb90',
    'Host': 'study.163.com',
    'Origin': 'http://study.163.com',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',

}

Payloads = {
    'activityId': 0,
    'frontCategoryId': "400000000158033",
    'orderType': 0,
    'pageIndex': 1,
    'pageSize': 50,
    'priceType': -1,
    'relativeOffset': 0,
    'searchTimeType': -1

}

fullinfo = []
# data=json.dumps(Payloads).encode(encoding='utf-8')


# content = Request(url, data=json.dumps(Payloads).encode(encoding='utf-8'), headers=headers)  # data是byte类型
# response = json.loads(urlopen(content).read().decode('UTF-8'))  # <class 'dict'>
# a = response['result']['list']
# print(a)


def GetCourses(url, headers=headers, Payload=Payloads):
    fullinfo = []
    iplist = ['http://106.14.51.145:8118', 'http://219.135.164.245:3128']
    random_ip = random.choice(iplist)
    print(random_ip)
    proxy_support = urllib.request.ProxyHandler({'http': random_ip})
    opener = urllib.request.build_opener(proxy_support, urllib.request.ProxyHandler)
    urllib.request.install_opener(opener)

    # data = urllib.request.urlopen(url).read().decode('utf-8', 'ignore')
    for i in range(1,17):
        Payload['pageIndex'] = i
        Payload['relativeOffset'] = 50 * i - 50

        content = Request(url, data=json.dumps(Payload).encode(encoding='utf-8'), headers=headers)
        response = json.loads(urllib.request.urlopen(content).read().decode('UTF-8'))


        # content = Request(url, data=json.dumps(Payload).encode(encoding='utf-8'),headers=headers)
        # response = json.loads(urlopen(content).read().decode('UTF-8'))
        fullinfo = fullinfo + response['result']['list']
        print("第{}页已抓取完毕".format(i))
        time.sleep(2)
    print("all page is OK!!!")
    return fullinfo


mydata = GetCourses(url)
myresult = pd.DataFrame(mydata)
# print(myresult)
myresult.info()

myresult=myresult.astype({'courseId':'str'})
#删除数据缺失列
print(myresult)
nouse=['bigImgUrl','activityIds','gmtModified','published','schoolShortName','tagIap','tagLectorTime','courseCardProps','displayType','endTime','imgUrl','productId','startTime']
myresult=myresult.drop(nouse, axis=1)
myresult.head(10)
#预览数据
myresult=myresult.set_index('courseId')
myresult.to_csv('D:\\a.csv',encoding='gbk')
