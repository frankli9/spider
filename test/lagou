import requests,random,json,time
from bs4 import BeautifulSoup

headers={
    'User-Agent':'Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding':'gzip, deflate',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Connection':'keep-alive',

    'X-Anit-Forge-Code':'0',
    'X-Anit-Forge-Token':'None',
    'X-Requested-With':'XMLHttpRequest',

}

proxy_list=[
    'http://113.123.19.152:808',
    'http://114.231.104.176:808',
    'http://119.39.68.61:808',
    ]
proxy_ip = random.choice(proxy_list)
proxies = {'http':proxy_ip}

# form_data={
#     'first':'true',
#     'pn':'2',
#     'kd':'python',
# }

def crawl_detail(id):
    url='https:/jobs/%s.html' % id
    # headers={

    #     'Upgrade-Insecure-Requests':'1',
    #     'User-Agent':'/537.36',
    # }
    req=requests.get(url,headers=headers,proxies=proxies)
    # print(req)
    soup=BeautifulSoup(req.text,'lxml')
    job_bt=soup.find('dd',attrs={'class':'job_bt'})
    time.sleep(5)
    return job_bt

def main():
    print(proxies)
    url='https:'
    positions=[]
    time.sleep(5)
    for x in range(1,31):
        form_data = {
            'first': 'true',
            'pn': x,
            'kd': 'python',
        }
        time.sleep(5)
        html=requests.post(url,data=form_data,proxies=proxies,headers=headers).json()
        print(html)
        page_positions=html['content']['positionResult']['result']
        for position in page_positions:
            position_dict={
                'position_name':position['positionName'],
                'work_year':position['workYear'],
                'salary':position['salary'],
                'district':position['district'],
                'company_name':position['companyFullName'],
            }
            position_id=position['positionId']
            position_detail=crawl_detail(position_id)
            position_dict['position_detail']=position_detail
            positions.append(position_dict)
        # print(type(positions)) #<class 'list'>
        # positions.extend(page_positions)
        time.sleep(5)





    line=json.dumps(positions,ensure_ascii=False)
    # print(type(line)) #<class 'str'>
    with open('D:\\python\\lagou-2.json','w+',encoding='utf-8') as fp:
        fp.write(line)
        #
        # print(line)

        # for position in positions:
        #     print('-'*50)
        #     print(position)


if __name__ == '__main__':
    main()
    # crawl_detail(3444178)

