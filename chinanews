
import requests,random,os,time,multiprocessing
from bs4 import BeautifulSoup
from lxml import etree
from selenium import webdriver
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

#模拟浏览器登录
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding':'gzip, deflate',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Cache-Control':'max-age=0',
    'Connection':'keep-alive',
    'Cookie':'__gads=ID=e90e92452442eef2:T=1497427490:S=ALNI_MZi4whQxIsWkk8QW0yrpnZ7MyTtfw; bdshare_firstime=1497599748961; PHPSESSID=jbnm2jk08p9fripuj44lu07427; yunsuo_session_verify=df3422073e655cde45e3dc9018fb2f94; Hm_lvt_adaf29565debc85c07b8d3c36c148a6b=1497604233,1497604246,1497604251,1497921441; Hm_lpvt_adaf29565debc85c07b8d3c36c148a6b=1497930371; AJSTAT_ok_pages=74; AJSTAT_ok_times=3',
}
#使用代理服务器
proxy_list=[
    'http://61.155.164.109:3128',  # 好的，目前能用
    # 'http://111.13.7.118:80',        #好的，快
    # 'http://106.14.51.145:8118', #9月25日测试，好的
    # 'http://111.13.7.119:80',
    # # 'http://111.13.7.123:80',
    # 'http://122.224.227.202:3128',
    # 'http://124.232.148.7:3128',
    # # 'http://218.201.98.196:3128',
    # 'http://219.135.164.245:3128',
    # 'http://123.56.169.22:3128',
    # 'http://222.189.228.59:3128',
    # 'http://222.189.238.47:3128',
    # 'http://222.189.238.3:3128',
    # 'http://61.155.164.109:3128',
    # 'http://222.189.228.60:3128',
    # 'http://222.189.228.58:3128',
    # 'http://124.64.32.187:9000',
    # 'http://113.65.8.221:9999',
    # 'http://222.189.228.62:3128',
    # 'http://222.189.228.41:3128',
    # 'http://120.26.14.14:3128',
    # 'http://123.56.169.22:3128',
    # 'http://123.139.56.238:9999',
    # 'http://113.200.214.164:9999',
    # 'http://61.155.164.109:3128',
    # 'http://61.155.164.112:3128',
    # 'http://113.200.159.155:9999',
    # 'http://101.37.79.125:3128',
    # 'http://59.44.244.14:9797',
    # 'http://219.149.59.250:9797',
    # 'http://125.46.0.62:53281',
    # 'http://124.64.32.187:9000',
    # 'http://14.116.153.16:3128',
    # 'http://118.254.142.42:53281',
    # 'http://203.174.112.13:3128',
    # 'http://125.45.87.12:9999',
    # 'http://61.155.164.106:3128',
    # 'http://123.138.89.133:9999',
    # 'http://119.90.63.3:3128',
    # 'http://60.191.134.165:9999',
    # 'http://58.252.6.165:9000',
    # 'http://113.65.8.221:9999',
    # 'http://115.203.197.16:808',
    # 'http://125.211.202.26:53281',
    # 'http://218.6.145.11:9797',
    # 'http://218.56.132.157:8080',
    # 'http://106.113.243.203:9999',
    # 'http://101.68.73.54:53281',
    # 'http://222.189.228.56:3128',
    # 'http://113.200.214.164:9999',
    # 'http://112.114.92.160:8118',
    # 'http://112.114.92.209:8118',
    # 'http://120.35.12.105:3128',
    # 'http://182.42.37.46:808',
    # 'http://123.138.89.133:9999',
    # 'http://122.114.122.212:9999',
    # 'http://222.189.228.3:3128',
    # 'http://113.121.241.72:808',
    # 'http://111.155.116.229:8123',
    # 'http://119.90.63.3:3128',
    # 'http://180.137.232.76:53281',
    # 'http://112.114.98.196:8118',


    ]
proxy_ip = random.choice(proxy_list)
proxies = {'http':proxy_ip}
print(proxy_ip)


second=[1,2,]
sleep_second = random.choice(second)



# url='http://www.chinanews.com/scroll-news/gn/2017/1220/news.shtml'
# index_url='http://www.chinanews.com/gn/2017/12-20/8404560.shtml'


def get_item_info(url):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--proxy-server=%s'%str(proxy_ip))
    chrome = webdriver.Chrome(chrome_options=chrome_options)
    chrome.get(url)
    location = WebDriverWait(chrome, 1).until(EC.presence_of_element_located((By.XPATH, './/div[@class="left_zw"]/p')))
    time.sleep(3)
    if location:
        try:
            a = chrome.find_elements_by_xpath('.//div[@class="left_zw"]/p')
            for a_1 in a:
                with open('E:\\Chinese\\刘翠项目\\数字符号20171220\\2017国内.txt', 'a', encoding='utf-8') as f:
                    f.write(a_1.text)
                    f.write('\n')
                    print(a_1.text)
        except:
            pass
    chrome.close()


# WebDriverWait(chrome, 20, 0.5).until(EC.presence_of_element_located(locator))
# WebDriverWait(chrome, 1, 0.5, ignored_exceptions=TimeoutException).until(lambda x: x.find_element_by_class_name("left_zw").is_displayed())

def get_links_from(page):
    urls=[]
    # list_view = 'http://www.tingvoa.com/oral/meitianyikeyingyukouyu365/index_{}.html'.format(str(page))
    list_view='http://www.chinanews.com/scroll-news/gn/2017/120{}/news.shtml'.format(str(page)) #http://auto.huanqiu.com/news/30.html
    wb_data = requests.get(list_view,headers=headers,proxies=proxies)
    soup=BeautifulSoup(wb_data.text,'lxml')
    # time.sleep(3.5)
    for link in soup.select('div.dd_bt > a'):
        item_link=link.get('href')
        urls.append(item_link)
        print(item_link)
    # return urls
    print(urls)
    print(len(urls))
    for i in urls:
        get_item_info(i)

for i in range(5,10):
    get_links_from(i)
    print('第 ',i,' 页爬取完')

#  多线程
#  if __name__=='__main__':
#      pool=multiprocessing.Pool()
#      pool.map(get_links_from,[i for i in range(1, 10)])


