
import requests,random,os,time,multiprocessing
from bs4 import BeautifulSoup
from lxml import etree
from selenium import webdriver
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

#模拟浏览器登录
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding':'gzip, deflate',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Cache-Control':'max-age=0',
    'Connection':'keep-alive',
}
#使用代理服务器
proxy_list=[



    ]
proxy_ip = random.choice(proxy_list)
proxies = {'http':proxy_ip}
print(proxy_ip)


second=[1,2,]
sleep_second = random.choice(second)



# url='http://www.chinanews.com/scroll-news/gn/2017/1220/news.shtml'
# index_url='http://www.chinanews.com/gn/2017/12-20/8404560.shtml'


def get_item_info(url):
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--proxy-server=%s'%str(proxy_ip))
    chrome = webdriver.Chrome(chrome_options=chrome_options)
    chrome.get(url)
    location = WebDriverWait(chrome, 1).until(EC.presence_of_element_located((By.XPATH, './/div[@class="left_zw"]/p')))
    time.sleep(3)
    if location:
        try:
            a = chrome.find_elements_by_xpath('.//div[@class="left_zw"]/p')
            for a_1 in a:
                with open('E:\\Chinese\\刘翠项目\\数字符号20171220\\2017国内.txt', 'a', encoding='utf-8') as f:
                    f.write(a_1.text)
                    f.write('\n')
                    print(a_1.text)
        except:
            pass
    chrome.close()


# WebDriverWait(chrome, 20, 0.5).until(EC.presence_of_element_located(locator))
# WebDriverWait(chrome, 1, 0.5, ignored_exceptions=TimeoutException).until(lambda x: x.find_element_by_class_name("left_zw").is_displayed())

def get_links_from(page):
    urls=[]
    # list_view = 'http://www.tingvoa.com/oral/meitianyikeyingyukouyu365/index_{}.html'.format(str(page))
    list_view='http://www.chinanews.com/scroll-news/gn/2017/120{}/news.shtml'.format(str(page)) #http://auto.huanqiu.com/news/30.html
    wb_data = requests.get(list_view,headers=headers,proxies=proxies)
    soup=BeautifulSoup(wb_data.text,'lxml')
    # time.sleep(3.5)
    for link in soup.select('div.dd_bt > a'):
        item_link=link.get('href')
        urls.append(item_link)
        print(item_link)
    # return urls
    print(urls)
    print(len(urls))
    for i in urls:
        get_item_info(i)

for i in range(5,10):
    get_links_from(i)
    print('第 ',i,' 页爬取完')

#  多线程
#  if __name__=='__main__':
#      pool=multiprocessing.Pool()
#      pool.map(get_links_from,[i for i in range(1, 10)])


