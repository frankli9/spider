import requests,random,os,time,json
from bs4 import BeautifulSoup
from lxml import etree
from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
import urllib.request
from urllib.request import urlopen, Request
import pandas as pd
import numpy as np

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding':'gzip, deflate',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Cache-Control':'max-age=0',
    'Connection':'keep-alive',
    'Cookie':'__gads=ID=e90e92452442eef2:T=1497427490:S=ALNI_MZi4whQxIsWkk8QW0yrpnZ7MyTtfw; bdshare_firstime=1497599748961; PHPSESSID=jbnm2jk08p9fripuj44lu07427; yunsuo_session_verify=df3422073e655cde45e3dc9018fb2f94; Hm_lvt_adaf29565debc85c07b8d3c36c148a6b=1497604233,1497604246,1497604251,1497921441; Hm_lpvt_adaf29565debc85c07b8d3c36c148a6b=1497930371; AJSTAT_ok_pages=74; AJSTAT_ok_times=3',
}

proxy_list=[

    ]

proxy_ip = random.choice(proxy_list)
proxies = {'http':proxy_ip}
print(proxy_ip)

urls=['http://pic{}'.format(str(x)) for x in range(0,481,48)]
n = 1
for url1 in urls:
    # print(url1)

    r=requests.get(url1,proxies=proxies,headers=headers).json()#.content.decode('utf-8')#.json(ensure_ascii=False) #.content.decode('gbk','ignore').encode('utf-8').
    pic_urls=r['items']
    for url2 in pic_urls:
        pic_url=url2['pic_url']
        with open('E:\\图片\\天空\\'+str(n) + '.png', 'wb+') as f:
            f.write(requests.get(pic_url, headers=headers,timeout=30,proxies=proxies).content)
            n += 1
        # print(pic_url)
        time.sleep(1)


