from bs4 import BeautifulSoup
from lxml import etree
import requests,multiprocessing,json,time,random

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',

}

proxy_list = [


]

proxy_ip =random.choice(proxy_list)
proxies = {'http':proxy_ip}


def get_item_info(url):

    html = requests.get(url,headers=headers,proxies=proxies).content.decode('utf-8','ignore').encode('utf-8')
    try:
        dom_tree = etree.HTML(html)
        words = dom_tree.xpath('.//div[@class="artContents"]//text()')

        with open('25.txt','a+',encoding='utf-8')as f:
            f.write('\n')
            for word in words:
                f.write(word)
                print(word)
    except:
        pass




def get_page_links(num):
    print('**************************', num, 'é¡µ******************************')
    print('****************************************************************************')
    urls = []
    url = 'pager={}'.format(num)
    wb_data = requests.get(url,headers=headers,proxies=proxies)
    soup = BeautifulSoup(wb_data.text,'lxml')
    page_urls = soup.select('div.NewsP > a')
    for url in page_urls:
        page_url = 'http://www.kexuehome.com' + url.get('href')
        urls.append(page_url)
        print(page_url)
    # print(urls)

    for url1 in urls:
        get_item_info(url1)
        time.sleep(2)
#
#
if __name__ == '__main__':
    pool = multiprocessing.Pool()
    pool.map(get_page_links,[i for i in range(1,26)])




