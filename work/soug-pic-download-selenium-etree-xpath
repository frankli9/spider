import requests,random,os,time
from bs4 import BeautifulSoup
from lxml import etree
from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy
from selenium.webdriver.common.proxy import ProxyType
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding':'gzip, deflate',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Cache-Control':'max-age=0',
    'Connection':'keep-alive',
    'Cookie':'__gads=ID=e90e92452442eef2:T=1497427490:S=ALNI_MZi4whQxIsWkk8QW0yrpnZ7MyTtfw; bdshare_firstime=1497599748961; PHPSESSID=jbnm2jk08p9fripuj44lu07427; yunsuo_session_verify=df3422073e655cde45e3dc9018fb2f94; Hm_lvt_adaf29565debc85c07b8d3c36c148a6b=1497604233,1497604246,1497604251,1497921441; Hm_lpvt_adaf29565debc85c07b8d3c36c148a6b=1497930371; AJSTAT_ok_pages=74; AJSTAT_ok_times=3',
}

proxy_list=[

    ]

proxy_ip = random.choice(proxy_list)
proxies = {'http':proxy_ip}
print(proxy_ip)

#测试图片单独页面
# def get_TOF(index_url):
#     time.sleep(3)
#     '''
#     在百度搜索python后，进入到python图片页面，在这个页面中下拉，选择所有的图片链接地址
#     '''
#     url_lists = []
#     chrome_options = webdriver.ChromeOptions()
#     chrome_options.add_argument('--proxy-server=%s'%str(proxy_ip))
#     chrome = webdriver.Chrome(chrome_options=chrome_options)
#     chrome.get(index_url)
#     chrome.implicitly_wait(5)
#     dom_tree = etree.HTML(chrome.page_source)
#     pic_url = list(dom_tree.xpath('//img[@log"]/@src'))[0]
#     print(pic_url)
#

#selenium-xpath
def mkdir(path):
    if not os.path.exists(path):
        os.mkdir(path)

def SavePic(filename,url):
    time.sleep(3)
    content_1 = requests.get(url,headers=headers,proxies=proxies).content
    with open(filename,'wb') as f:
        f.write(content_1)
        print(url,'保存完毕')

def get_TOF(index_url):
    time.sleep(3)
    '''
    在百度搜索python后，进入到python图片页面，在这个页面中下拉，选择所有的图片链接地址
    '''
    url_lists = []
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--proxy-server=%s'%str(proxy_ip))
    chrome = webdriver.Chrome(chrome_options=chrome_options)
    chrome.get(index_url)
    chrome.implicitly_wait(5)
    for i in range(1,5):
        chrome.execute_script('window.scrollTo(0,document.body.scrollHeight)')
        time.sleep(2)
    # print(chrome.page_source)
    dom_tree = etree.HTML(chrome.page_source)
    pic_links = dom_tree.xpath('.//a[@class="picContainer"]/@href')

    for link in pic_links:
        wancheng_link='http://'+link
        # print(wancheng_link)
        if link not in url_lists:
            url_lists.append(wancheng_link)

    # print(len(url_list))
    return url_lists

def get_pic(url_lists):
    time.sleep(3)
    '''
    打开每个图片地址的url，
    找到图片的地址，
    并写入到本地
    '''
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--proxy-server=%s' % str(proxy_ip))
    chrome = webdriver.Chrome(chrome_options=chrome_options)
    n=1
    for url in url_lists:
        chrome.get(url)
        print(url)
        chrome.implicitly_wait(3)
        print(chrome.page_source)
        print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')
        dom_tree = etree.HTML(chrome.page_source)
        # if len(list(dom_tree.xpath('//img[@class="currentImg"]/@src'))[0]) < 1:
        #     pic_url = list(dom_tree.xpath('.//*[@id="srcPic"]/img/@src'))[0]
        # else:
        #     pic_url = list(dom_tree.xpath('.//*[@id="currentImg"]/@src'))[0]
        pic_url = list(dom_tree.xpath('//*[@id="imageBox"]/img/@src'))[0]
        filename = str(n) + '.png'
        SavePic(filename, pic_url)
        time.sleep(1)
        n += 1
    chrome.quit()

def main():
    index_url = 'http://'
    Comics1 = get_TOF(index_url)
    get_pic(Comics1)
if __name__ == '__main__':
    main()

